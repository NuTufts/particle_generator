{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/kseuro/Kai/deeplearnphysics/pytorch/particle_generator/')\n",
    "\n",
    "# My stuff\n",
    "import ae\n",
    "import utils\n",
    "from dataloader import LArCV_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the GPU (GPU 1 is the best option)\n",
    "device = torch.device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the root path of the AutoEncoder experiments folder\n",
    "exp_root = \"/media/hdd1/kai/particle_generator/experiments/larcv_ae/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "\n",
      " Exp_0: /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256 \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " Exp_1: /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_20_15000-epochs \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " Exp_2: /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_20 \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " Exp_3: /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " Exp_4: /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_128_20 \n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get the names of all the experiments in the exp_root folder\n",
    "exp_paths = []\n",
    "for path in os.listdir(exp_root):\n",
    "    exp_paths.append(os.path.join(exp_root, path))\n",
    "\n",
    "print(\"-\"*60)\n",
    "for i in range(len(exp_paths)):\n",
    "    print(\"\\n Exp_{}:\".format(str(i)), exp_paths[i], '\\n')\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dir of the particular experiment to be deployed\n",
    "exp_dir = exp_paths[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment path set as: \n",
      "/media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/\n"
     ]
    }
   ],
   "source": [
    "# Create the full path to the experiment\n",
    "exp_path = os.path.join(exp_root, exp_dir) + \"/\"\n",
    "print(\"Experiment path set as: \\n{}\".format(exp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config csv as a dict\n",
    "config_csv = exp_path + \"config.csv\"\n",
    "config_df = pd.read_csv(config_csv, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model architecture from config df\n",
    "n_layers = int(config_df[config_df['Unnamed: 0'].str.contains(\"n_layers\")==True]['0'].item())\n",
    "l_dim    = int(config_df[config_df['Unnamed: 0'].str.contains(\"l_dim\")==True]['0'].item())\n",
    "im_size  = int(config_df[config_df['Unnamed: 0'].str.contains(\"dataset\")==True]['0'].item())**2\n",
    "im_dim   = int(np.sqrt(im_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to model weights\n",
    "weights_dir = \"weights/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 test data will be loaded from: \n",
      "/media/hdd1/kai/particle_generator/larcv_data/test/larcv_png_64/\n"
     ]
    }
   ],
   "source": [
    "test_data = \"/media/hdd1/kai/particle_generator/larcv_data/test/larcv_png_{}/\".format(im_dim)\n",
    "num_test_ex = sum( [len(examples) for _, _, examples in os.walk(test_data)] )\n",
    "print(\"{} test data will be loaded from: \\n{}\".format(num_test_ex, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data will be loaded from: \n",
      "/media/hdd1/kai/particle_generator/larcv_data/train/larcv_png_64/\n"
     ]
    }
   ],
   "source": [
    "# Path to the training data\n",
    "train_data = \"/media/hdd1/kai/particle_generator/larcv_data/train/larcv_png_{}/\".format(im_dim)\n",
    "print(\"Training data will be loaded from: \\n{}\".format(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image conversion flag is: L\n",
      "Images will be loaded from subfolder of: /media/hdd1/kai/particle_generator/larcv_data/test/larcv_png_64/\n",
      "Image conversion flag is: L\n",
      "Images will be loaded from subfolder of: /media/hdd1/kai/particle_generator/larcv_data/train/larcv_png_64/\n"
     ]
    }
   ],
   "source": [
    "loader_kwargs = {'num_workers' : 2, 'batch_size': 1, 'shuffle': True}\n",
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],[0.5])])\n",
    "test_dataset    = LArCV_loader(root = test_data, transforms = transforms)\n",
    "train_dataset   = LArCV_loader(root = train_data, transforms = transforms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, **loader_kwargs)\n",
    "train_loader = DataLoader(train_dataset, **loader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AE layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = [256] \n",
    "\n",
    "# Compute encoder sizes\n",
    "sizes = lambda: [ (yield 2**i) for i in range(n_layers) ]\n",
    "enc_sizes = base * n_layers\n",
    "enc_sizes = [a*b for a,b in zip(enc_sizes, [*sizes()])][::-1]\n",
    "\n",
    "# Update kwarg dicts\n",
    "# Decoder is the reverse of the encoder\n",
    "ae_kwargs = {'enc_sizes' : enc_sizes, 'l_dim' : l_dim, 'im_size' : im_size, 'dec_sizes' : enc_sizes[::-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the model loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of MSE results for train and test set for each checkpoint\n",
    "MSE_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get checkpoint name(s)\n",
    "checkpoint_path  = exp_path + weights_dir\n",
    "checkpoint_names = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    checkpoint_names.append(os.path.join(checkpoint_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "\n",
      " 0 : /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/weights/best_ae_ep_600.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " 1 : /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/weights/best_ae_ep_700.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " 2 : /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/weights/best_ae_ep_900.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " 3 : /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/weights/best_ae_ep_650.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " 4 : /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/weights/best_ae_ep_950.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " 5 : /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/weights/best_ae_ep_750.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " 6 : /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/weights/best_ae_ep_999.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " 7 : /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/weights/best_ae_ep_850.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      " 8 : /media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/weights/best_ae_ep_800.tar \n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*60)\n",
    "for i in range(len(checkpoint_names)):\n",
    "    print(\"\\n {} :\".format(str(i)), checkpoint_names[i], '\\n')\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_checkpoint = checkpoint_names[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "# Keys: ['state_dict', 'epoch', 'optimizer']\n",
    "checkpoint = torch.load(current_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model on GPU\n",
    "model = ae.AutoEncoder(**ae_kwargs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model's state dictionary\n",
    "# Note: The IncompatibleKeys(missing_keys=[], unexpected_keys=[]) message indicates that\n",
    "#       there were no problems in loading the state dictionary. Bit confusing...\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Encoder(\n",
       "    (fc_blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (last): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc_blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (last): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over the test data and record the average loss for the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, image in enumerate(test_loader):\n",
    "    # Flatten image and copy to gpu\n",
    "    image = image.view(loader_kwargs['batch_size'], -1).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    \n",
    "    # Get the loss value for the batch\n",
    "    loss = loss_fn(output, image)\n",
    "    \n",
    "    # Append loss value\n",
    "    test_losses.append(float(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_loss = np.mean(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.116 \n"
     ]
    }
   ],
   "source": [
    "print(\"Average test loss: {} \".format( round(avg_test_loss, 3) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over the training data and record the average loss for the checkpoint\n",
    "- Here, we only loop over the same number of training examples as there are test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, image in enumerate(train_loader):    \n",
    "    # Flatten image and copy to gpu\n",
    "    image = image.view(loader_kwargs['batch_size'], -1).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    \n",
    "    # Get the loss value for the batch\n",
    "    loss = loss_fn(output, image)\n",
    "    \n",
    "    # Append loss value\n",
    "    train_losses.append(float(loss.item()))\n",
    "    \n",
    "    if (idx + 1) == num_test_ex:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_loss = np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.1 \n"
     ]
    }
   ],
   "source": [
    "print(\"Average train loss: {} \".format( round(avg_train_loss, 3) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the difference between the test loss and train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_MSE = abs( avg_train_loss - avg_test_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_MSE: 0.017\n",
      "For checkpoint: \n",
      "/media/hdd1/kai/particle_generator/experiments/larcv_ae/larcv_ae_64_256_1000-epochs/weights/best_ae_ep_999.tar\n"
     ]
    }
   ],
   "source": [
    "print(\"delta_MSE: {}\".format( round(delta_MSE, 3)))\n",
    "print('For checkpoint: \\n{}'.format(current_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append the results to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [avg_test_loss, avg_train_loss, delta_MSE, current_checkpoint]\n",
    "MSE_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_MSE: 0.01205\n",
      "delta_MSE: 0.01345\n",
      "delta_MSE: 0.01402\n",
      "delta_MSE: 0.01547\n",
      "delta_MSE: 0.01479\n",
      "delta_MSE: 0.01593\n",
      "delta_MSE: 0.01696\n",
      "delta_MSE: 0.01561\n",
      "delta_MSE: 0.01684\n"
     ]
    }
   ],
   "source": [
    "## Checkpoint Analysis\n",
    "for i in range(len(MSE_results)):\n",
    "    print(\"delta_MSE: {}\".format(round(MSE_results[i][2], 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save evaluation results to experiment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = exp_path + \"checkpoint_evaluation.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_file, 'w+') as file_object:\n",
    "    for exp in MSE_results:\n",
    "        test_loss  = exp[0]\n",
    "        train_loss = exp[1]\n",
    "        delta_MSE  = exp[2]\n",
    "        checkpoint = exp[3]\n",
    "        line1 = \"Checkpoint: {}\\n\".format(checkpoint)\n",
    "        line2 = \"test_loss: {}, train_loss: {}, delta_MSE: {}\".format(test_loss, train_loss, delta_MSE)\n",
    "        line = line1 + line2 + \"\\n\"\n",
    "        file_object.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
