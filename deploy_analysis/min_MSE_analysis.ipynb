{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/kseuro/Kai/deeplearnphysics/pytorch/particle_generator/')\n",
    "\n",
    "# My stuff\n",
    "import ae\n",
    "import conv_ae\n",
    "import utils\n",
    "from dataloader import LArCV_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the GPU (GPU 1 is the best option)\n",
    "device = torch.device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the root path of the AutoEncoder experiments folder\n",
    "exp_root = \"/media/hdd1/kai/particle_generator/experiments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model type folder (either MLP or Conv model)\n",
    "model_folder = \"larcv_ae/\" + \"conv_ae/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the exp_root\n",
    "exp_root += model_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the names of all the experiments in the exp_root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "\n",
      " Exp_0: /media/hdd1/kai/particle_generator/experiments/larcv_ae/conv_ae/conv_ae_64_1000-epochs \n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "exp_paths = []\n",
    "for path in os.listdir(exp_root):\n",
    "    exp_paths.append(os.path.join(exp_root, path))\n",
    "\n",
    "print(\"-\"*60)\n",
    "for i in range(len(exp_paths)):\n",
    "    print(\"\\n Exp_{}:\".format(str(i)), exp_paths[i], '\\n')\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dir of the particular experiment to be deployed\n",
    "exp_dir = exp_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment path set as: \n",
      "/media/hdd1/kai/particle_generator/experiments/larcv_ae/conv_ae/conv_ae_64_1000-epochs/\n"
     ]
    }
   ],
   "source": [
    "# Create the full path to the experiment\n",
    "exp_path = os.path.join(exp_root, exp_dir) + \"/\"\n",
    "print(\"Experiment path set as: \\n{}\".format(exp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config csv as a dict\n",
    "config_csv = exp_path + \"config.csv\"\n",
    "config_df = pd.read_csv(config_csv, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model architecture from config df\n",
    "n_layers = int(config_df[config_df['Unnamed: 0'].str.contains(\"n_layers\")==True]['0'].values.item())\n",
    "l_dim    = int(config_df[config_df['Unnamed: 0'].str.contains(\"l_dim\")==True]['0'].values.item())\n",
    "im_size  = int(config_df[config_df['Unnamed: 0'].str.contains(\"dataset\")==True]['0'].values.item())**2\n",
    "depth    = int(config_df[config_df['Unnamed: 0'].str.contains(\"depth\")==True]['0'].values.item())\n",
    "im_dim   = int(np.sqrt(im_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to model weights\n",
    "weights_dir = \"weights/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 test data will be loaded from: \n",
      "/media/hdd1/kai/particle_generator/larcv_data/test/larcv_png_64/\n"
     ]
    }
   ],
   "source": [
    "test_data = \"/media/hdd1/kai/particle_generator/larcv_data/test/larcv_png_{}/\".format(im_dim)\n",
    "num_test_ex = sum( [len(examples) for _, _, examples in os.walk(test_data)] )\n",
    "print(\"{} test data will be loaded from: \\n{}\".format(num_test_ex, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to reference training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 reference training data will be loaded from: \n",
      "/media/hdd1/kai/particle_generator/larcv_data/train/train_reference/larcv_png_64/\n"
     ]
    }
   ],
   "source": [
    "# Path to the reference training data\n",
    "train_data = \"/media/hdd1/kai/particle_generator/larcv_data/train/train_reference/larcv_png_{}/\".format(im_dim)\n",
    "num_train_ex = sum( [len(examples) for _, _, examples in os.walk(train_data)] )\n",
    "print(\"{} reference training data will be loaded from: \\n{}\".format(num_train_ex, train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image conversion flag is: L\n",
      "Images will be loaded from subfolder of: /media/hdd1/kai/particle_generator/larcv_data/test/larcv_png_64/\n",
      "Image conversion flag is: L\n",
      "Images will be loaded from subfolder of: /media/hdd1/kai/particle_generator/larcv_data/train/train_reference/larcv_png_64/\n"
     ]
    }
   ],
   "source": [
    "loader_kwargs = {'num_workers' : 2, 'batch_size': 1, 'shuffle': True}\n",
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],[0.5])])\n",
    "test_dataset    = LArCV_loader(root = test_data,  transforms = transforms)\n",
    "train_dataset   = LArCV_loader(root = train_data, transforms = transforms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, **loader_kwargs)\n",
    "train_loader = DataLoader(train_dataset, **loader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AE layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up AE layer sizes\n",
    "if 'mlp' in exp_path:\n",
    "    base = [256] \n",
    "\n",
    "    # Compute encoder sizes\n",
    "    sizes = lambda: [ (yield 2**i) for i in range(n_layers) ]\n",
    "    enc_sizes = base * n_layers\n",
    "    enc_sizes = [a*b for a,b in zip(enc_sizes, [*sizes()])][::-1]\n",
    "\n",
    "    # Update kwarg dicts\n",
    "    # Decoder is the reverse of the encoder\n",
    "    ae_kwargs = {'enc_sizes' : enc_sizes, 'l_dim' : l_dim, 'im_size' : im_size, 'dec_sizes' : enc_sizes[::-1]}\n",
    "else:\n",
    "    depth   = [depth] * n_layers\n",
    "    divisor = lambda: [ (yield 2**i) for i in range(n_layers) ]\n",
    "    depth   = [a//b for a,b in zip(depth, [*divisor()])][::-1]\n",
    "    ae_kwargs = {'enc_depth' : depth, 'dec_depth' : depth[::-1] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the model loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get checkpoint name(s)\n",
    "checkpoint_path  = exp_path + weights_dir\n",
    "checkpoint_names = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    checkpoint_names.append(os.path.join(checkpoint_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "\n",
      "0: best_conv_ae_ep_650.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "1: best_conv_ae_ep_600.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "2: best_conv_ae_ep_999.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "3: best_conv_ae_ep_700.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "4: best_conv_ae_ep_850.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "5: best_conv_ae_ep_900.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "6: best_conv_ae_ep_950.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "7: best_conv_ae_ep_750.tar \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "8: best_conv_ae_ep_800.tar \n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*60)\n",
    "for i in range(len(checkpoint_names)):\n",
    "    name = checkpoint_names[i].split('/')[-1]\n",
    "    checkpoint_name_labels.append(name)\n",
    "    print(\"\\n{}:\".format(str(i)), name, '\\n')\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "current_checkpoint = checkpoint_names[index]\n",
    "current_checkpoint_label = checkpoint_name_labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "# Keys: ['state_dict', 'epoch', 'optimizer']\n",
    "checkpoint = torch.load(current_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model on GPU\n",
    "if 'mlp' in exp_path:\n",
    "    model = ae.AutoEncoder(**ae_kwargs).to(device)\n",
    "else:\n",
    "    model = conv_ae.ConvAutoEncoder(**ae_kwargs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model's state dictionary\n",
    "# Note: The IncompatibleKeys(missing_keys=[], unexpected_keys=[]) message indicates that\n",
    "#       there were no problems in loading the state dictionary. Bit confusing...\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvAutoEncoder(\n",
       "  (encoder): ConvEncoder(\n",
       "    (conv_blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvDecoder(\n",
       "    (deconv_blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): ConvTranspose2d(4, 1, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over the test data and record the losses for the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT RE-RUN THIS CELL ##\n",
    "\n",
    "# Keep track of MSE results for train and test set for each checkpoint\n",
    "MSE_results = []\n",
    "\n",
    "## DO NOT RE-RUN THIS CELL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, image in enumerate(test_loader):\n",
    "    \n",
    "    # Flatten image into a vector\n",
    "    if 'mlp' in exp_path:\n",
    "        image = image.view(loader_kwargs['batch_size'], -1).to(device)\n",
    "    else:\n",
    "        image = image.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    \n",
    "    # Get the loss value for the batch\n",
    "    loss = loss_fn(output, image)\n",
    "    \n",
    "    # Append loss value\n",
    "    test_losses.append(float(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the average loss over segments of the test_loss results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 100\n",
    "for step in range(0, len(test_losses), step_size):\n",
    "    avg_loss.append( np.mean(test_losses[step:step+step_size]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append the average losses for checkpoint to MSE_results list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_results.append( (current_checkpoint_label, avg_loss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('best_conv_ae_ep_600.tar',\n",
       " [0.051372345606796446,\n",
       "  0.04990461527602747,\n",
       "  0.05336467694956809,\n",
       "  0.0590628065308556,\n",
       "  0.0529610536666587,\n",
       "  0.048975902246311306,\n",
       "  0.05392821523826569,\n",
       "  0.060506708920001984,\n",
       "  0.049383576116524636,\n",
       "  0.03905495669227094,\n",
       "  0.0521879989723675,\n",
       "  0.0531745119066909,\n",
       "  0.04021610344294459,\n",
       "  0.057216570707969365,\n",
       "  0.04706878937315196,\n",
       "  0.048417820539325473,\n",
       "  0.054501020684838294,\n",
       "  0.05520246678031981,\n",
       "  0.04814299096353352,\n",
       "  0.05329641030519269,\n",
       "  0.04740869264584035,\n",
       "  0.05108458469854668,\n",
       "  0.050948390811681746,\n",
       "  0.059271647478453815,\n",
       "  0.04571080099674873,\n",
       "  0.05269901586929336,\n",
       "  0.050862777039874345,\n",
       "  0.05327076094923541,\n",
       "  0.051250235582701864,\n",
       "  0.05340486356988549,\n",
       "  0.05113966939970851,\n",
       "  0.04561732641654089,\n",
       "  0.05431932720821351,\n",
       "  0.05425816401140764,\n",
       "  0.05528076795861125,\n",
       "  0.05117770564742386,\n",
       "  0.04302377099636942,\n",
       "  0.05232137248385697,\n",
       "  0.060067517287097874,\n",
       "  0.05726561586372554,\n",
       "  0.050156978766899556,\n",
       "  0.04416209883522242,\n",
       "  0.04624473638832569,\n",
       "  0.041680357456207275,\n",
       "  0.05220443277154118,\n",
       "  0.04798854944063351,\n",
       "  0.0551917561609298,\n",
       "  0.060773374405689536,\n",
       "  0.052720134132541716,\n",
       "  0.060156139403115955,\n",
       "  0.048091951690148564,\n",
       "  0.05229295950382948,\n",
       "  0.057258042758330706,\n",
       "  0.042088281807955354,\n",
       "  0.04798889291007072,\n",
       "  0.04296333919977769,\n",
       "  0.04158516607247293,\n",
       "  0.04673069699667394,\n",
       "  0.054957831385545435,\n",
       "  0.05065278740366921,\n",
       "  0.054339659209363164,\n",
       "  0.05255754535086453,\n",
       "  0.05782982079777867,\n",
       "  0.04514181696576998,\n",
       "  0.04713063082657754,\n",
       "  0.05205754260532558,\n",
       "  0.056730620455928145,\n",
       "  0.05914719197433442,\n",
       "  0.060608566449955104,\n",
       "  0.05299604756059125,\n",
       "  0.04351153237279504,\n",
       "  0.053813341837376356,\n",
       "  0.05003833354916423,\n",
       "  0.044703940278850496,\n",
       "  0.05189924796111882,\n",
       "  0.05321090754587203,\n",
       "  0.04543102327734232,\n",
       "  0.05445830277167261,\n",
       "  0.052134526073932645,\n",
       "  0.051676062755286696,\n",
       "  0.04539256130810827,\n",
       "  0.04721896673087031,\n",
       "  0.059160611818078905,\n",
       "  0.04530397483846173,\n",
       "  0.04439250563737005,\n",
       "  0.055560659379698334,\n",
       "  0.05883551042992621,\n",
       "  0.05627039429964498,\n",
       "  0.05330285613425076,\n",
       "  0.05002144115976989,\n",
       "  0.05243691806681454,\n",
       "  0.04360457816859707,\n",
       "  0.04479557334445417,\n",
       "  0.06037178659113124,\n",
       "  0.05141642635688186,\n",
       "  0.060675305030308664,\n",
       "  0.053706727062817664,\n",
       "  0.04320692749228328,\n",
       "  0.04805509109050035,\n",
       "  0.05034618660341948])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(MSE_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over the training data and record the average loss for the checkpoint\n",
    "- Here, we only loop over the REFERENCE training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, image in enumerate(train_loader):    \n",
    "\n",
    "    # Flatten image into a vector\n",
    "    if 'mlp' in exp_path:\n",
    "        image = image.view(loader_kwargs['batch_size'], -1).to(device)\n",
    "    else:\n",
    "        image = image.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    \n",
    "    # Get the loss value for the batch\n",
    "    loss = loss_fn(output, image)\n",
    "    \n",
    "    # Append loss value\n",
    "    train_losses.append(float(loss.item()))\n",
    "    \n",
    "    if (idx + 1) == num_test_ex:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_loss = np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average train loss: {} \".format( round(avg_train_loss, 3) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the difference between the test loss and train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_MSE = abs( avg_train_loss - avg_test_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"delta_MSE: {}\".format( round(delta_MSE, 3)))\n",
    "print('For checkpoint: \\n{}'.format(current_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append the results to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [avg_test_loss, avg_train_loss, delta_MSE, current_checkpoint]\n",
    "MSE_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checkpoint Analysis\n",
    "for i in range(len(MSE_results)):\n",
    "    print(\"delta_MSE: {}\".format(round(MSE_results[i][2], 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save evaluation results to experiment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = exp_path + \"checkpoint_evaluation.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_file, 'w+') as file_object:\n",
    "    for exp in MSE_results:\n",
    "        test_loss  = exp[0]\n",
    "        train_loss = exp[1]\n",
    "        delta_MSE  = exp[2]\n",
    "        checkpoint = exp[3]\n",
    "        line1 = \"Checkpoint: {}\\n\".format(checkpoint)\n",
    "        line2 = \"test_loss: {}, train_loss: {}, delta_MSE: {}\".format(test_loss, train_loss, delta_MSE)\n",
    "        line = line1 + line2 + \"\\n\" # Punctuation with a newline character is a Unix best practice\n",
    "        file_object.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
